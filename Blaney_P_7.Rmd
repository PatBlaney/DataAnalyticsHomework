---
title: "R Notebook"
output:
  pdf_document:
    latex_engine: xelatex
---
# Packages needed for analysis:
tidyverse
knitr
rvest
(Additionally, I used the Google Chrome extension of SelectorGadget to identify the CSS
selector for each part being scraped)
```{r}
library(tidyverse)
library(knitr)
library(rvest)
```

# Part A.
# I will be using Import.io and rvest 

# Save URLs of pages 1-3 for the search. The criteria for the search is to find burger places
# specifically in Allston, Brighton, Back Bay, Beacon Hill, Downtown, Fenway, South End, and
# West End.
```{r}
burgerSeachPage1 <- read_html("https://www.yelp.com/search?find_desc=Burgers&start=0&l=p:MA:Boston::%5BAllston/Brighton,Back_Bay,Beacon_Hill,Downtown,Fenway,South_End,West_End%5D")

burgerSeachPage2 <- read_html("https://www.yelp.com/search?find_desc=Burgers&start=10&l=p:MA:Boston::%5BAllston/Brighton,Back_Bay,Beacon_Hill,Downtown,Fenway,South_End,West_End%5D")

burgerSeachPage3 <- read_html("https://www.yelp.com/search?find_desc=Burgers&start=20&l=p:MA:Boston::%5BAllston/Brighton,Back_Bay,Beacon_Hill,Downtown,Fenway,South_End,West_End%5D")
```

# Extract information from list of restaurants on search page
# First, extract names of restaurants from each page
```{r}
restaurantNamePage1 <- burgerSeachPage1 %>%
  html_nodes(".indexed-biz-name") %>%
  html_text() %>%
  as.vector()

restaurantNamePage2 <- burgerSeachPage2 %>%
  html_nodes(".indexed-biz-name") %>%
  html_text() %>%
  as.vector()

restaurantNamePage3 <- burgerSeachPage3 %>%
  html_nodes(".indexed-biz-name") %>%
  html_text() %>%
  as.vector()

allRestaurantNames <- c(restaurantNamePage1, restaurantNamePage2, restaurantNamePage3) %>%
  str_replace_all("^\\d.[ .*]+|(\\n)", "") %>%
  as_vector()
```

# Second, extract their addresses
```{r}
addressesPage1 <- burgerSeachPage1 %>%
  html_nodes(".natural-search-result address") %>%
  html_text()

addressesPage2 <- burgerSeachPage2 %>%
  html_nodes(".natural-search-result .secondary-attributes :nth-child(2)") %>%
  html_text()

addressesPage3 <- burgerSeachPage3 %>%
  html_nodes(".natural-search-result address") %>%
  html_text()

allAddresses <- c(addressesPage1, addressesPage2, addressesPage3) %>%
  str_replace_all("(\\n) +", "") %>%
  as_vector()
```

# Third, extract their review star rating
```{r}
reviewStarRatingPage1 <- burgerSeachPage1 %>%
  html_nodes(".natural-search-result .rating-large") %>%
  html_attrs() %>%
  as.data.frame() %>%
  t() %>%
  as_tibble() %>%
  select("title")

reviewStarRatingPage2 <- burgerSeachPage2 %>%
  html_nodes(".natural-search-result .rating-large") %>%
  html_attrs() %>%
  as.data.frame() %>%
  t() %>%
  as_tibble() %>%
  select("title")

reviewStarRatingPage3 <- burgerSeachPage3 %>%
  html_nodes(".natural-search-result .rating-large") %>%
  html_attrs() %>%
  as.data.frame() %>%
  t() %>%
  as_tibble() %>%
  select("title")

allStarRatings <- c(
  reviewStarRatingPage1$title, reviewStarRatingPage2$title, reviewStarRatingPage3$title
  ) %>%
  as_vector()
```

# Fourth, extract the review count
```{r}
reviewCountPage1 <- burgerSeachPage1 %>%
  html_nodes(".natural-search-result .rating-qualifier") %>%
  html_text()

reviewCountPage2 <- burgerSeachPage2 %>%
  html_nodes(".natural-search-result .rating-qualifier") %>%
  html_text()

reviewCountPage3 <- burgerSeachPage3 %>%
  html_nodes(".natural-search-result .rating-qualifier") %>%
  html_text()

allReviewCounts <- c(reviewCountPage1, reviewCountPage2, reviewCountPage3) %>%
  str_replace_all("(\\n) +", "") %>%
  as_vector()
```

# Finally, extract the service category
```{r}
serviceCategoryPage1 <- burgerSeachPage1 %>%
  html_nodes(".natural-search-result .price-category") %>%
  html_text() %>%
  str_replace_all("[\\n$]", "") %>%
  str_replace_all("( ){2,}", "") %>%
  str_replace_all(",", ", ")

serviceCategoryPage2 <- burgerSeachPage2 %>%
  html_nodes(".natural-search-result .price-category") %>%
  html_text() %>%
  str_replace_all("[\\n$]", "") %>%
  str_replace_all("( ){2,}", "") %>%
  str_replace_all(",", ", ")

serviceCategoryPage3 <- burgerSeachPage3 %>%
  html_nodes(".natural-search-result .price-category") %>%
  html_text() %>%
  str_replace_all("[\\n$]", "") %>%
  str_replace_all("( ){2,}", "") %>%
  str_replace_all(",", ", ")

allServiceCategories <- c(
  serviceCategoryPage1, serviceCategoryPage2, serviceCategoryPage3
  ) %>%
  as_vector()
```

# Part B.
# Complete dataframe with all 30 observations
```{r}
topBurgerPlacesBoston <- tibble(
  "Restuarant Name" = allRestaurantNames,
  "Ratings" = allStarRatings,
  "Total Number of Reviews" = allReviewCounts,
  "Address" = allAddresses,
  "Service Categories" = allServiceCategories
)
```

# Part C.
# Comparison between methods: rvest vs Import.io
The first major difference would be accessability and cost. The rvest package within R is 
totally free where as I had to use a 7 day trial of Import.io, otherwise I would have had
to purchase the software. 
Once allowed use of the software, it is clear why Import.io has a good reputation for web
scraping. Just by simply providing the website URL, it parsed, near perfectly, all the data
that was of interest for this assignment.
```{r}
include_graphics("images/import_screenshot.png")
```

Navigating the software took a little while to understand and it wasnt always clear what
was going on. After playing around with the software, I did eventually extract the data and
was able to download the information into CSV file which I could then run through R for 
additional downstream analysis. 
```{r}
include_graphics("images/import_screenshot_output.png")
```
I then turned to giving rvest a try and it was very straight
forward. This is largely in part of my familiarity with the language but also the
simplicity and repeatability of the code. Ultimately, I picked rvest because I had an easier
time troubleshooting and narrowing the scope of interest but, after putting in more time 
using the Import.io software, I think I would change my mind.

# Part D.
After looking at the URLs of the three pages, the only difference I see is in the 
'&start=' part. There is a number displayed after this line corresponding to the index of
the first restaurant listed. For example, the second page has the first listed number as
11 so the index is 10. When looking at the URL, you see '&start=10'.
All parameters I selected are also present in the URL. Since I restricted the search to 
locations in Allston, Brighton, Back Bay, Beacon Hill, Downtown, Fenway, South
End, and WestEnd, you can see these all listed after the 'l=' which in this case is 
followed by reference to the state and greater area (MA:Boston).
In this instance, it seems that the parameters for pagination are the amount of displayed 
restaurants per page.
My guess would be https://www.yelp.com/search?find_desc=Chinese&start=60&l=p:NY:New_York::
